{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7xNlxpABWd1"
   },
   "source": [
    "<h1 align=\"left\">Inferential Statistics </h1>\n",
    "<br>\n",
    "\n",
    "![](https://drive.google.com/uc?id=1ujrKN6hY4RHYl0NXM8KDNH0b2t8uVmm7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vVYCURVBWd2"
   },
   "source": [
    "## Agenda for the Day\n",
    "***\n",
    "\n",
    "- Introduction to Inference\n",
    "- Central Limit Theorem\n",
    "- Hypothesis Testing\n",
    "- P-value and Confidence Interval\n",
    "- Type I and Type II error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1592557444692,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "xz-cAFxzBWd2",
    "outputId": "99fa3bec-54c9-4db7-e28a-3dbc7722b277"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29KqDD4NBWd6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95daAB7nBWd9"
   },
   "source": [
    "# Introduction to Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tiabkpo0BWd-"
   },
   "source": [
    "## Random Experiment (1/2)\n",
    "***\n",
    "- John has a dataset on the Sale Prices of 1460 Houses in Brooklyn and is curious to experiment with his data. \n",
    "\n",
    "- He somehow thinks that 1460 is a lot of houses to look at! So he needs to narrow his search down a bit.\n",
    "\n",
    "- What he does is naive but he decides to just pick 500 houses at random. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5Y2KD_2BWd-"
   },
   "source": [
    "## Random Experiment (2/2)\n",
    "***\n",
    "\n",
    "- John has a feeling that these houses might not truly represent the prices and features of the houses he can select fron the dataset.\n",
    "- In simple words, he thought that these houses may be either too expensive, or too cheap as compared to the 500 \n",
    "   - What he means is: The mean of the 500 houses would be pretty far away from the mean of the 1460 houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdaWHGTFBWd-"
   },
   "source": [
    "## John's 1st Simple Test\n",
    "***\n",
    " - So John tests the mean of the 500 houses and compares it to the 1460 houses\n",
    " \n",
    " - What John has done, is nothing but build the basic concepts of Statistical Inference in himself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1592570635181,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "kdIH5wV-BWd_"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "sample_ages = np.random.choice(a= data['SalePrice'], size=500) # Sample 500 values\n",
    "\n",
    "# Print sample mean\n",
    "\n",
    "\n",
    "# Print population mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3H7-YtapBWeC"
   },
   "source": [
    "## Statistical Inference\n",
    "***\n",
    " - Let's try and understand the words \"Sample\" & \"Population\" as these will be used a lot in Statistics\n",
    " \n",
    " - Again, let's build our intuition with the help of some easy examples \n",
    " \n",
    " \n",
    " - While analyzing data with statistical thinking, we are often interested in the characteristics of some large population\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "advWEEStBWeD"
   },
   "source": [
    "## Statistical Inference\n",
    "***\n",
    "\n",
    "- But **collecting data on the entire population may be infeasible**\n",
    " \n",
    "      - For example, leading up to U.S. presidential elections it could be very useful to know the political leanings of every single eligible voter, but surveying every voter is not feasible.\n",
    "      - Instead, we could poll some subset of the population, such as a thousand registered voters, and use that data to make inferences about the population as a whole.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UocYV1k9BWeD"
   },
   "source": [
    "## Statistical Inference\n",
    "***\n",
    " - This *\"subset\"* of the population is nothing but the **Sample** data \n",
    " \n",
    " - We carry out various tests on the Sample to gain insight on the larger population out there! \n",
    " \n",
    " - Therefore Statistical inference is the process of analyzing sample data to gain insight into the population from which the data was collected and to investigate differences between data samples.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQugDLSsBWeD"
   },
   "source": [
    "## Statistical Inference\n",
    "***\n",
    "- In John's case, he is primarily concerned with the SalePrice of every house in Brooklyn \n",
    "    - Therefore, it's safe to assume that John's 1460 house prices is a **Sample** of the data \n",
    "    - The Population would be the price of **Every** house in Brooklyn \n",
    "    \n",
    "\n",
    " - But since John took a subset (i.e. 500) of the data from 1460 we can assume that: \n",
    "     - The 1460 Houses are the Population for the next few examples \n",
    "     - The 500 houses are the Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J28IYN54BWeE"
   },
   "source": [
    "## Some Terminology: Point Estimates\n",
    "***\n",
    "Point estimates are estimates of population parameters based on sample data. \n",
    "\n",
    "* For instance, if we wanted to know the average age of registered voters in the U.S., we could take a survey of registered voters and then use the average age of the respondents as a point estimate of the average age of the population as a whole.\n",
    "\n",
    "* The sample mean is usually not exactly the same as the population mean. This difference can be caused by many factors including poor survey design, biased sampling methods and the randomness inherent to drawing a sample from a population. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N45gIvfqBWeE"
   },
   "source": [
    "## Point Estimates in John's Simple Test \n",
    "***\n",
    "* John's point estimate based on a sample of 500 houses underestimates the true population mean by \\$3,400, but it is close! \n",
    "\n",
    "\n",
    "* This illustrates an important point: *we can get a fairly accurate estimate of a large population by sampling a relatively small subset of individuals*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpwEAGpBBWeF"
   },
   "source": [
    "## Terminology - Parameter & Statistic \n",
    "***\n",
    " - A parameter is a descriptive measure of the population.\n",
    "    - Example: Population mean, Population variance etc.\n",
    "- A statistic is a  descriptive measure of the sample.\n",
    "    - Example: Sample mean, Sample variance etc.\n",
    "     ***\n",
    "![](https://drive.google.com/uc?id=1Uk2oPyLURh8we2oWsuWKV_TOfE-lotTt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO90iiSEBWeF"
   },
   "source": [
    " ***\n",
    "![](https://drive.google.com/uc?id=1eRBel16Ll66CQxbACCZdq1JW3DgNP22g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRibV606BWeG"
   },
   "source": [
    "## Don't try this at home\n",
    " ***\n",
    "![](https://drive.google.com/uc?id=1QIEZSWL87BDR6JbTEjQMF4U-gIbDZH4p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKMIAk8wBWeG"
   },
   "source": [
    "#### Quiz : Inferential statistics enable you to\n",
    "\n",
    "A. decide if the research hypothesis is true.\n",
    "\n",
    "B. decide if the null hypothesis is false. \n",
    "\n",
    "C. estimate population parameters. \n",
    "\n",
    "D. decide if your research results are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsMoOslUBWeH"
   },
   "source": [
    "## Some Basic Concepts! \n",
    "***\n",
    " - Before moving on to other 'experiments' John performed with his data, let's try and uderstand some of the most commonly used concepts in Statistics\n",
    " \n",
    " - You might have heard about the term Central Limit Theorem before\n",
    " \n",
    " - Let's understand what it means and its relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEJz2PgpBWeH"
   },
   "source": [
    "### Central Limit Theorem  - Important! (1/2) \n",
    "***\n",
    " - The central limit theorem (CLT) is a statistical theory that states that given a sufficiently large sample size from a population with a finite level of variance, the `mean` of all samples from the same population will be approximately equal to the `mean` of the population. Furthermore, all of the samples will follow an approximate normal distribution pattern, with all variances being approximately equal to the variance of the population divided by each sample's size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKDaEPv_BWeI"
   },
   "source": [
    "### Central Limit Theorem  - Important! (2/2)\n",
    "***\n",
    "- According to the central limit theorem, the mean of a sample of data will be closer to the mean of the overall population in question as the sample size increases, notwithstanding the actual distribution of the data, and whether it is normal or non-normal. As a general rule, sample sizes equal to or greater than 30 are considered sufficient for the central limit theorem to hold, meaning the distribution of the sample means is fairly normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MApWI8auBWeM"
   },
   "source": [
    "\n",
    "## Central Limit Theorem - Important (1/2)\n",
    "***\n",
    "* Many practices in statistics, such as those involving hypothesis testing or confidence intervals, make some assumptions concerning the population that the data was obtained from.\n",
    "\n",
    "* One assumption that is initially made in a statistics course is that the populations that we work with are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lA4p3irABWeN"
   },
   "source": [
    "\n",
    "## Central Limit Theorem - Important (2/2)\n",
    "***\n",
    "\n",
    "* The assumption that data is from a normal distribution simplifies matters but seems a little unrealistic.\n",
    "\n",
    "* Just a little work with some real-world data shows that outliers, skewness, multiple peaks and asymmetry show up quite routinely like the one John encountered with `SalePrice` above.\n",
    "\n",
    "* We can get around the problem of data from a population that is not normal. \n",
    "\n",
    "* The use of an appropriate sample size and the central limit theorem help us to get around the problem of data from populations that are not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7121,
     "status": "ok",
     "timestamp": 1592570648395,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "0wco_cBbBWeN",
    "outputId": "2d2efe58-fba0-456d-c1a2-c42b8a767847"
   },
   "outputs": [],
   "source": [
    "from functools import partial # provides capability to define function with partial arguments\n",
    "\n",
    "N=1000 # number of times n samples are taken. Try varying this number.\n",
    "nobb=101 # number of bin boundaries on plots\n",
    "n=np.array([1,2,3,5,10,100, 200]) # number of samples to average over\n",
    "\n",
    "exp_mean=3 # mean of exponential distribution\n",
    "a,b=0.7,0.5 # parameters of beta distribution\n",
    "\n",
    "dist=[partial(np.random.random),partial(np.random.exponential,exp_mean),partial(np.random.beta,a,b)]\n",
    "title_names=[\"Flat\", \"Exponential (mean=%.1f)\" % exp_mean, \"Beta (a=%.1f, b=%.1f)\" % (a,b)]\n",
    "drange=np.array([[0,1],[0,10],[0,1]]) # ranges of distributions\n",
    "means=np.array([0.5,exp_mean,a/(a+b)]) # means of distributions\n",
    "var=np.array([1/12,exp_mean**2,a*b/((a+b+1)*(a+b)**2)]) # variances of distributions\n",
    "\n",
    "binrange=np.array([np.linspace(p,q,nobb) for p,q in drange])\n",
    "ln,ld=len(n),len(dist)\n",
    "plt.figure(figsize=((ld*4)+1,(ln*2)+1))\n",
    "\n",
    "for i in range(ln): # loop over number of n samples to average over\n",
    "    for j in range(ld): # loop over the different distributions\n",
    "        plt.subplot(ln,ld,i*ld+1+j)\n",
    "        plt.hist(np.mean(dist[j]((N,n[i])),1),binrange[j])\n",
    "        plt.xlim(drange[j])\n",
    "        if j==0:\n",
    "            plt.ylabel('n=%i' % n[i],fontsize=15)        \n",
    "        if i==0:\n",
    "            plt.title(title_names[j], fontsize=15)\n",
    "        else:\n",
    "            clt=(1/(np.sqrt(2*np.pi*var[j]/n[i])))*np.exp(-(((binrange[j]-means[j])**2)*n[i]/(2*var[j])))\n",
    "            plt.plot(binrange[j],clt,'y',linewidth=2)     \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UPxyrFIBWeP"
   },
   "source": [
    "In the graphs above the yellow curve is the predicted Gaussian distribution from the Central Limit Thereom. Notice that the rate of convergence of the sample mean to the Gaussian depends on the original parent distribution. Also, \n",
    "\n",
    "- the mean of the Gaussian distribution is the same as the original parent distribution,\n",
    "- the width of the Gaussian distribution scales as $1/\\sqrt{n}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eA354HK1BWeQ"
   },
   "source": [
    "## In Class Activity\n",
    "***\n",
    "In order to understand the Central Limit Theorem and understand why distribution of  sample means is normally distributed, try tinkering around the below shiny app. Change the Parent Distribution, sample size and no of samples and see how CLT helps us make inferences about the population statistics even when the population isn't normal.\n",
    "\n",
    "\n",
    "https://gallery.shinyapps.io/CLT_mean/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKvVS61uBWeQ"
   },
   "source": [
    "## In Class Activity\n",
    "***\n",
    "\n",
    "We have a dataset which contains a list of Monthly Home Sale Prices in a market. Since we are looking at all of the selling prices within the market. Calculate the population mean for the given sale prices.\n",
    "\n",
    "\n",
    "$250000, $175000, $325000, $185000, $450000, $275000, $255000, $320000, $310000, $120000, $280000\n",
    "\n",
    "- Calculate the sample mean for first 3 sale prices.\n",
    "- Also, calculate the sample mean for the next three sale prices\n",
    "- And compare sample means with the population mean.\n",
    "- What if we calculate the sample mean multiple times, will the average of all sample means be closer to the population mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1592570718932,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "XKXm1IGaBWeR"
   },
   "outputs": [],
   "source": [
    "a = [250000,175000,325000, 185000,  450000, 275000,  255000, 320000,  310000, 120000, 280000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1592557926082,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "65F58P6PBWeT"
   },
   "outputs": [],
   "source": [
    "# Calculate the sample mean for first 3 sale prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 981,
     "status": "ok",
     "timestamp": 1592557932681,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "ym04s9AiBWeX"
   },
   "outputs": [],
   "source": [
    "# Calculate the sample mean for the next three sale prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1592557939088,
     "user": {
      "displayName": "Satyapriya Chaudhari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwqGWH8rItAiQcXo2XqlnRlLD_bHd12hqXzQ0qlg=s64",
      "userId": "15909927639307960054"
     },
     "user_tz": -330
    },
    "id": "kVmPf0s6BWec"
   },
   "outputs": [],
   "source": [
    "# Compare sample means with the population mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXc4LhmqBWef"
   },
   "outputs": [],
   "source": [
    "# What if we calculate the sample mean multiple times, will the average of all sample means be closer to the population mean?\n",
    "\n",
    "import random \n",
    "\n",
    "no_of_sample = [5, 10, 15, 20]\n",
    "avg_of_means = {}\n",
    "\n",
    "for i in no_of_sample:\n",
    "    sample_means = []\n",
    "    for j in range(1,i+1):\n",
    "        # generate a random sample of size 3\n",
    "        \n",
    "        \n",
    "        # Calculate the mean of the samples and append to sample_means\n",
    "        \n",
    "        \n",
    "    # append the means of sample_means to avg_of_means\n",
    "    \n",
    "    \n",
    "# print(avg_of_means)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9_LXuxvBWeh"
   },
   "source": [
    "#### Quiz : A distribution of sample means is a collection of sample means from\n",
    "\n",
    "A. samples of different sizes from different populations. \n",
    "\n",
    "B. samples of size N from different populations. \n",
    "\n",
    "C. samples of size N from the same population. \n",
    "\n",
    "D. samples of different sizes from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjcggyE6BWei"
   },
   "source": [
    "## Degrees of Freedom \n",
    "***\n",
    "-  central to the principle of estimating statistics of populations from samples of them. \n",
    "-  commonly abbreviated to df.\n",
    "-  a mathematical restriction that needs to be put in place when estimating one statistic from an estimate of another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfCrl5TPBWei"
   },
   "source": [
    "## Understanding Degrees of Freedom\n",
    "***\n",
    "\n",
    "The easiest way to understand Degrees of Freedom conceptually is through an example:\n",
    "\n",
    "\n",
    "- Consider a data sample consisting of, for the sake of simplicity, five positive integers. The values could be any number with no known relationship between them. This data sample would, theoretically, have five degrees of freedom.\n",
    "- Four of the numbers in the sample are {3, 8, 5, and 4} and the average of the entire data sample is revealed to be 6.\n",
    "- This must mean that the fifth number has to be 10. It can be nothing else. It does not have the freedom to vary.\n",
    "- So the Degrees of Freedom for this data sample is 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScgNfBvaBWej"
   },
   "source": [
    "## John's Second Experiment \n",
    "***\n",
    " - Now John wants to test that if he takes 1000 houses, instead of 500, will those houses contain the Population (1460 Houses) mean too? \n",
    " \n",
    " - He thinks that it might not contain the Population mean and if so, he'll have to take another sample of 1000. So how many times would he have to do this? \n",
    " \n",
    " - Thanks to programming, it's easy to take many samples. But he want's to be confident that the Population Mean is contained in his samples, majority of the times. \n",
    " \n",
    " - Confused? That's ok. John basically wants to check the confidence interval of the Mean of his many samples of 1000 houses! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qE9MCEPNBWej"
   },
   "source": [
    "\n",
    "## Confidence Intervals  (1/4)\n",
    "***\n",
    " - A Confidence Interval is a range of values we are fairly sure our true value lies in! \n",
    " \n",
    " - That's basically the simplest but true explanation of Confidence Interval\n",
    "     ***\n",
    "![](https://drive.google.com/uc?id=1coH-twv73WX-Pi7ZgNFYmt0VTrxouNY_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYpxJ8LzBWek"
   },
   "source": [
    "## Confidence Intervals (2/4)\n",
    "***\n",
    " - Let's build our intuition with the help of an example\n",
    " \n",
    " - Example: Average Height\n",
    "\n",
    "    - We measure the heights of 40 randomly chosen men, and get a:\n",
    "\n",
    "            - mean height of 175cm,\n",
    "            - with a standard deviation of 20cm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXDcri8PBWek"
   },
   "source": [
    "## Confidence Intervals (3/4)\n",
    "***\n",
    " - The 95% Confidence Interval (we will learn how to calculate this later) is:\n",
    " ***\n",
    "![](https://drive.google.com/uc?id=18UmG0CnNKIw8Hx6TS7E6_AFXjBbNn7Kd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_z_3TMmBWek"
   },
   "source": [
    "## Confidence Intervals (4/4)\n",
    "***\n",
    "- This says the true mean of ALL men (if we could measure their heights) is likely to be between 168.8cm and 181.2cm.\n",
    "\n",
    "- But it might not be!\n",
    "\n",
    "- The \"95%\" says that 95% of experiments like we just did will include the true mean, but 5% won't.\n",
    "\n",
    "- So there is a 1-in-20 chance (5%) that our Confidence Interval does NOT include the true mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOCfcxoyBWel"
   },
   "source": [
    "## Calculating the Confidence Interval (1/3)\n",
    "***\n",
    "Step 1: note down the number of samples n, and calculate the mean X and standard deviation s of those samples:\n",
    "\n",
    " - Number of samples: n = 40\n",
    " - Mean: X_bar = 175\n",
    " - Standard Deviation: s = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjI7c29uBWel"
   },
   "source": [
    "## Calculating the Confidence Interval (2/3)\n",
    "***\n",
    "Step 2: decide what Confidence Interval we want. 90%, 95% and 99% are common choices. Then find the \"Z\" value for that Confidence Interval here:\n",
    " ***\n",
    " ![](https://drive.google.com/uc?id=1XftZW5iPbibhnnUnkY0cSHPccegEGlG7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEBji-38BWem"
   },
   "source": [
    "## Calculating the Confidence Interval (3/3)\n",
    "***\n",
    "For 95% the Z value is 1.960\n",
    "\n",
    "Step 3: use that Z in this formula for the Confidence Interval\n",
    " ***\n",
    " ![](https://drive.google.com/uc?id=1aK7Si7jG2iZSpwJBye5YIXhYlcMkWPOk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9aju7E2BWem"
   },
   "source": [
    "Thus, we have: \n",
    "    \n",
    "    175 ± 1.960\\*(20/sqrt(40))\n",
    "    \n",
    "    = 175cm ± 6.2cm\n",
    "    \n",
    "In other words: from 168.8cm to 181.2cm\n",
    "\n",
    " - The value after the ± is called the **margin of error**\n",
    " - The **margin of error** in the previous example is 6.20cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJ_CEWUwBWen"
   },
   "source": [
    "## Interpretation of Confidence Intervals\n",
    "***\n",
    "Let's build our intution, again, with the help of an example. This should help explain how to interpret the Confidence Interval\n",
    "\n",
    "Example: Apple Orchard\n",
    "\n",
    " - Are the apples big enough?\n",
    "\n",
    " - There are hundreds of apples on the trees, so you randomly choose just 30 and get these results:\n",
    "\n",
    "    - Mean: 86\n",
    "    - Standard Deviation: 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yju2oH5BWen"
   },
   "source": [
    "## Interpretation of Confidence Intervals\n",
    "***\n",
    "We know:\n",
    "\n",
    " - X is the mean = 86\n",
    " - Z is the Z-value = 1.960 (from the table above for 95%)\n",
    " - s is the standard deviation = 5\n",
    " - n is the number of samples = 30\n",
    " - 86  ±  1.960\t5\t = 86 ± (1.79/√30)\n",
    "\n",
    "\n",
    "So the true mean (of all the hundreds of apples) is likely to be between 84.21 and 87.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9uhGzg0BWen"
   },
   "source": [
    "## Interpretation of Confidence Intervals\n",
    "***\n",
    "True Mean\n",
    "\n",
    "Now imagine we get to pick ALL the apples straight away, and get them ALL measured by the packing machine (this is a luxury not normally found in statistics!)\n",
    "\n",
    "And the true mean turns out to be 84.9\n",
    "\n",
    "Let's lay all the apples on the ground from smallest to largest:\n",
    "\n",
    " - Each apple is a green dot, except our samples which are blue\n",
    " ***\n",
    " ![](https://drive.google.com/uc?id=192dXR1uyl07nj2yz3s6txWTnerZWEVC_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYo5Y74BBWeo"
   },
   "source": [
    "## Interpretation of Confidence Intervals\n",
    "***\n",
    " - Our result was not exact ... it is random after all ... but the true mean is inside our confidence interval of 86 ± 1.79 (in other words 84.21 to 87.79) \n",
    " \n",
    " - But the true mean might not be inside the confidence interval but 95% of the time it will!\n",
    " \n",
    "  ### 95% of all \"95% Confidence Intervals\" will include the true mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rbg9J3bmBWep"
   },
   "source": [
    "## Interpretation of Confidence Intervals (1/2)\n",
    "***\n",
    "Maybe we had this sample, with a mean of 83.5 and a Standard Deviation of 3.5:\n",
    "\n",
    " - That does not include the true mean. Expect that to happen 5% of the time for a 95% confidence interval.\n",
    "\n",
    " - So how do we know if the sample we took is one of the \"lucky\" 95% or the unlucky 5%? Unless we get to measure the whole population like above we simply don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfFQKkbFBWep"
   },
   "source": [
    "## Interpretation of Confidence Intervals (1/2)\n",
    "***\n",
    "- *This is the risk in sampling, we might have a bad sample.* (Important!) \n",
    " \n",
    " - There are various Statistical techniques that are done to specifically address this issue (Discussion is beyond the scope of this lecture) \n",
    "\n",
    "\n",
    "***\n",
    " ![](https://drive.google.com/uc?id=192dXR1uyl07nj2yz3s6txWTnerZWEVC_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuvHrVXyBWeq"
   },
   "source": [
    "## We should try and keep it real but laziness..\n",
    "\n",
    "***\n",
    " ![](https://drive.google.com/uc?id=1gvv7qc3SscapjF6UhixzEA4-tzk7qMiZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM0Hqhm_BWer"
   },
   "source": [
    "## Coming Back to John's Experiment \n",
    "***\n",
    " - John decides to carry out the experiment to see whether the houses do have the True Mean within the sample! \n",
    "\n",
    " - Let's get right into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dx-qeVlFBWer"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "sample_size = 1000\n",
    "sample = np.random.choice(a= data['SalePrice'], size = sample_size)\n",
    "sample_mean = sample.mean()\n",
    "\n",
    "# Get the z-critical value*\n",
    "z_critical = stats.norm.ppf(q = 0.95)  \n",
    "\n",
    "# Check the z-critical value\n",
    "print(\"z-critical value:\")              \n",
    "print(z_critical)                        \n",
    "\n",
    "# Get the population standard deviation\n",
    "\n",
    "\n",
    "# calculate the margin_of_error\n",
    "\n",
    "\n",
    "# Calculate confidence_interval\n",
    "\n",
    "\n",
    "# Print confidence_interval\n",
    "\n",
    "\n",
    "# Print True Mean of the population to check if it lies in the interval\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhEcjDe4BWet"
   },
   "source": [
    "* Notice that the true mean is contained in our interval.\n",
    "* A confidence interval of 95% would mean that if we take many samples and create confidence intervals for each of them, 95% of our sample's confidence intervals will contain the true population mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfV7Owx7BWet"
   },
   "source": [
    "## John's Experiment - Continued\n",
    "***\n",
    "* Now, instead of 1, John creates several confidence intervals and plots them to get a better sense of what it means to \"capture\" the true mean\n",
    "\n",
    "* He does this for 25 trials! \n",
    "\n",
    "* Let's check out what he got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTmn43UPBWeu"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "intervals = [] # To store the confidence intervals\n",
    "sample_means = [] # To store the sample_means\n",
    "\n",
    "for sample in range(25):\n",
    "    sample = np.random.choice(a= data['SalePrice'], size = sample_size)\n",
    "    sample_mean = sample.mean()\n",
    "    sample_means.append(sample_mean)\n",
    "\n",
    "    # Get the z-critical value*         \n",
    "    z_critical = stats.norm.ppf(q = 0.975)  \n",
    "\n",
    "    # Get the population standard deviation\n",
    "    \n",
    "    stats.norm.ppf(q = 0.025)\n",
    "    # Calculate the margin_of_error\n",
    "    \n",
    "\n",
    "    # Calculate the confidence_inetrval\n",
    "\n",
    "\n",
    "    # append the confidence_interval to intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6O-5kEzBWev"
   },
   "outputs": [],
   "source": [
    "# Plot of the intervals to check if Population Mean lies in the interval\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "\n",
    "plt.errorbar(x=np.arange(0.1, 25, 1), \n",
    "             y=sample_means, \n",
    "             yerr=[(top-bot)/2 for top,bot in intervals],\n",
    "             fmt='o')\n",
    "\n",
    "plt.hlines(xmin=0, xmax=25,\n",
    "           y=data['SalePrice'].mean(), \n",
    "           linewidth=2.0,\n",
    "           color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLIeZaBVBWex"
   },
   "source": [
    "### Notice that in the plot above, all but one of the 95% confidence intervals overlap the red line marking the true mean. This is to be expected: since a 95% confidence interval captures the true mean 95% of the time, we'd expect our interval to miss the true mean 5% of the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-Ku0gUxBWey"
   },
   "source": [
    "## John's Curiosity of OldTown \n",
    "***\n",
    " - Now, John is interested in dissecting the whole Real Estate Scene in OldTown\n",
    " \n",
    " - He's interested in seeing whether the prices of houses are different (on an average) when compared to the rest of Brooklyn \n",
    " \n",
    " - John has just conjured up what is famously known as a \"Hypothesis\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeMdoQv1BWey"
   },
   "source": [
    "## Hypothesis \n",
    "***\n",
    "A statement that might be true, which can then be tested.\n",
    "\n",
    "Example: Sam has a hypothesis that \"large dogs are better at catching tennis balls than small dogs\". We can test that hypothesis by having hundreds of different sized dogs try to catch tennis balls.\n",
    "\n",
    "- The beauty of these Hypotheses are that they can be TESTED! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exSbOGKoBWey"
   },
   "source": [
    "## Hypothesis Testing \n",
    "***\n",
    "- Statistical hypothesis tests are based a statement called the null hypothesis that assumes nothing interesting is going on between whatever variables you are testing. \n",
    " \n",
    " - Therefore, in John's case the Null Hypothesis is that:\n",
    "     - \"The Mean of House Prices in OldTown is not different from the Houses all over Brooklyn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NobYJ6GABWez"
   },
   "source": [
    "## Why Null Hypothesis? \n",
    "***\n",
    " - The purpose of a hypothesis test is to determine whether the null hypothesis is likely to be true given sample data.\n",
    " - If there is little evidence against the null hypothesis given the data, you accept the null hypothesis.\n",
    " - If the null hypothesis is unlikely given the data, you might reject the null in favor of the alternative hypothesis: that something interesting is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yh2hKA4pBWez"
   },
   "source": [
    "## Alternative Hypothesis\n",
    "***\n",
    " - This is nothing but the question you ask which kind of \"opposes\" the Null Hypothesis\n",
    " \n",
    " - Therefore, in John's case the Alternative Hypothesis is that:\n",
    "     - \"The Mean of House Prices in OldTown **IS** different from the Houses all over Brooklyn\n",
    "     \n",
    " - Only 1 Hypothesis can be right\n",
    " \n",
    " - In hypothesis testing we test a sample, with the goal of accepting or rejecting a null hypothesis which is our assumption or the default position. The test tells us whether or not our primary hypothesis is true.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GeIiJO3BWez"
   },
   "source": [
    "## Important\n",
    "***\n",
    "\n",
    "### The null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis\n",
    "\n",
    " - We require a standard on the available evidence to reject the null hypothesis (convict)\n",
    "\n",
    "\n",
    "If we set a low standard\n",
    ", then we would increase the percentage of innocent people convicted\n",
    "; however we would also increase the percentage of guilty people convicted\n",
    "(correctly rejecting the null)\n",
    "\n",
    "\n",
    "If we set a high standard, then we increase the the percentage of innocent people let free\n",
    " while we would also increase the percentage of guilty people let free\n",
    "(type II errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5ubOMd_BWe0"
   },
   "source": [
    "## Math behind Hypothesis Testing (1/2)\n",
    "***\n",
    "Once you have the null and alternative hypothesis in hand, you choose a significance level (often denoted by the Greek letter α). \n",
    "\n",
    " - The significance level is a probability threshold that determines when you reject the null hypothesis.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEyrvne0BWe0"
   },
   "source": [
    "## Math behind Hypothesis Testing (2/2)\n",
    "***\n",
    "- So we use this to calculate a \"Test Statistic\" that would further help us do further calculations\n",
    " ***\n",
    " ![](https://drive.google.com/uc?id=1UGFvzznJ2unrH14h1OzOdtTuJQBJJOMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_BoKNDqBWe0"
   },
   "source": [
    "## Math behind Hypothesis Testing \n",
    "***\n",
    " - After carrying out a test, if the probability of getting a result as extreme as the one you observe due to chance is lower than the significance level, you reject the null hypothesis in favor of the alternative. \n",
    " \n",
    " - This probability of seeing a result as extreme or more extreme than the one observed is known as the *p-value*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZylS5KiLBWe1"
   },
   "source": [
    "## Interpretation of p-value - EASY! (1/2)\n",
    "***\n",
    "- The p-value is really not as complicated as people make it sound\n",
    "\n",
    "- So now say that we have put a significance (alpha) = 0.05\n",
    "    - This means that if we see a p-value of lesser than 0.05, we reject our Null and accept the Alternative to be true \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCEW8Ea_BWe2"
   },
   "source": [
    "## Interpretation of p-value - EASY! (2/2)\n",
    "***\n",
    "\n",
    "- What you have to understand is the data from your Null hypothesis follows a distribution (Normally distributed) \n",
    "     - Just imagine 1 bell curve of the data from the Null Hypothesis\n",
    "     - Now imagine another bell curve which hypothetically defines your Alternative Hypothesis\n",
    "     \n",
    "     See below: \n",
    "        \n",
    " ***\n",
    " ![](https://drive.google.com/uc?id=1rTUO1uwltYpIGlamL5nPtzM_8mR8wq3w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_QybGNCBWe3"
   },
   "source": [
    "## Interpretation of p-value - EASY! (1/2)\n",
    "***\n",
    " - So what the p-value says is that it is the Probability of finding the Alternative Hypothesis data in the Null Hypothesis data (bell curve 1!!) \n",
    " \n",
    " - If it is lesser than 0.05(our threshold) then we reject it\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaIU8iu0BWe3"
   },
   "source": [
    "## Interpretation of p-value - EASY! (2/2)\n",
    "***\n",
    "Why reject it though? \n",
    "\n",
    " - BECAUSE OUR ALTERNATIVE HYPOTHESIS DATA IS REAL! NO ONE MADE IT UP! IT IS LEGIT DATA THAT IS OBSERVED AND NOT JUST FAKE! \n",
    " - So if it is real, we can say that such data isn't really described by the Null Hypothesis (Bell Curve 1) therefore the Null must be rejected as being TRUE! \n",
    " \n",
    " - It now makes sense! P-values are cool again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oisdeB2HBWe4"
   },
   "source": [
    "#### Quiz : Statistical inference is the part of hypothesis testing that\n",
    "\n",
    "A. helps you to prove the null hypothesis is false. \n",
    "\n",
    "B. helps you to determine the probability that a sample is from one population or another. \n",
    "\n",
    "C. helps you do determine if the research hypothesis is powerful. \n",
    "\n",
    "D. helps you to determine if the research hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrKdVHgxBWe5"
   },
   "source": [
    "## Coming back to John \n",
    "***\n",
    " - Let's see how John did now\n",
    " - Are house prices in OldTown really different from the House Prices of Brooklyn? \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRwg-aX6BWe5"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Calculate the z_statistics and p_value using the ztest function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aC4TnSt5BWe7"
   },
   "source": [
    "## Summary of the p-value\n",
    "***\n",
    "* When performing a hypothesis test, the p-value is the probability of given or more extreme outcome given the null-hypothesis is true.\n",
    "\n",
    "* We see that the p-value is close to zero i.e., the probability of getting the given distribution of houseprices in OldTown under the assumption that it its mean is the same as the mean of all house prices.\n",
    "* So what can we infer from the p-value of our test? What should be the p-value beyond which we reject the null hypothesis.\n",
    "* The p-value below which we reject our hypothesis depends on our **significance level** $\\alpha$\n",
    "* For a 95% signifigance level we reject our null hypothesis if p-value is below 0.05\n",
    "* In this case we can reject the null hypothesis at 95% significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yh4yrKCLBWe8"
   },
   "source": [
    "## Another way to test: Gosset's (Student's) t-test\n",
    "***\n",
    "* The T-test is a statistical test used to determine whether a numeric data sample of differs significantly from the population or whether two samples differ from one another.\n",
    "* A z-test assumes a sample size >30 to work, but what if our sample is less than 30?\n",
    "* A t-test solves this problem and gives us a way to do a hypothesis test on a smaller sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ost9pouFBWe8"
   },
   "source": [
    "Now, John also wants to see if house prices in `Stone Brook` neighborhood are different from the rest of the Houses in Brooklyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCTWJNvNBWe8"
   },
   "outputs": [],
   "source": [
    "# Print the Number of Houses in Stone Brook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXxNPbcHBWe-"
   },
   "source": [
    "Lets do a t-test to test our hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qq2RKD00BWe-"
   },
   "outputs": [],
   "source": [
    "# Calculate the t_statistics and p_value using the ttest_1samp function of the stats module\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIhkqrD8BWfA"
   },
   "source": [
    "* The p-value in this case again is low and we can reject our null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU14FwzgBWfB"
   },
   "source": [
    "### Type I and Type II Error\n",
    "***\n",
    "* If we again think of hypothesis test as a criminal trial then it makes sense to frame the verdict in terms of null and alternate hypothesis:\n",
    "\n",
    "    * Null Hypothesis: Defendant is innocent\n",
    "    * Alternate Hypothesis: Defendant is guilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgWZbG0CBWfB"
   },
   "source": [
    "### Type I and Type II Error\n",
    "***\n",
    "* What type of error is being committed in the following circumstances?\n",
    "    * Declaring the defendant guilty when they are actually innocent?\n",
    "    * Declaring the defendant innocent when they are actually guilty?\n",
    "    \n",
    "* The first one is a type I error also known as a \"false positive\" or \"false hit\".\n",
    "* The type 1 error rate is equal to the significance level α, so setting a higher confidence level (and therefore lower alpha) reduces the chances of getting a false positive.\n",
    "\n",
    "* The second one is a type I error also known as a \"false negative\" or \"miss\". The higher your confidence level, the more likely you are to make a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnAxiC_kBWfB"
   },
   "source": [
    " ***\n",
    " ![](https://drive.google.com/uc?id=1GtJgLYOCqSGwvLFgE09MDPj_jttexIbj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHHpBw64BWfC"
   },
   "source": [
    " ***\n",
    "![](https://drive.google.com/uc?id=1N_eJb_fhpiaxYz7lfYsLzUfcOQp28LOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1ngLrrJBWfC"
   },
   "source": [
    "## Type 1 Error \n",
    "***\n",
    "Type I error describes a situation where you reject the null hypothesis when it is actually true. \n",
    "\n",
    "This type of error is also known as a false positive or false hit.\n",
    "\n",
    "The type 1 error rate is equal to the significance level α, so setting a higher confidence level (and therefore lower alpha) reduces the chances of getting a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUKTx2zWBWfC"
   },
   "source": [
    "## Type 2 error\n",
    "***\n",
    "Type II error describes a situation where you fail to reject the null hypothesis when it is actually false. \n",
    "\n",
    "Type II error is also known as a false negative or miss. The higher your confidence level, the more likely you are to make a type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYhWCD0pBWfD"
   },
   "source": [
    "### Chi-Squared Goodness-Of-Fit Test (1/2)\n",
    "***\n",
    "A chi-squared goodness of fit tests whether the distribution of sample categorical data matches an expected distribution. \n",
    "\n",
    "* For example, you could use a chi-squared goodness-of-fit test to check whether the race demographics of members at your church or school match that of the entire U.S. population or whether the computer browser preferences of your friends match those of Internet uses as a whole.\n",
    "\n",
    "When working with categorical data the values the observations themselves aren't of much use for statistical testing because categories like \"male\", \"female,\" and \"other\" have no mathematical meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjJccSPRBWfD"
   },
   "source": [
    "### Chi-Squared Goodness-Of-Fit Test (2/2)\n",
    "***\n",
    "Tests dealing with categorical variables are based on variable counts instead of the actual value of the variables themselves.\n",
    "\n",
    "Let's generate some fake demographic data for U.S. and Minnesota and walk through the chi-square goodness of fit test to check whether they are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBwI9HPRBWfE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "national = pd.DataFrame([\"white\"]*100000 + [\"hispanic\"]*60000 +\\\n",
    "                        [\"black\"]*50000 + [\"asian\"]*15000 + [\"other\"]*35000)          \n",
    "\n",
    "minnesota = pd.DataFrame([\"white\"]*600 + [\"hispanic\"]*300 + \\\n",
    "                         [\"black\"]*250 +[\"asian\"]*75 + [\"other\"]*150)\n",
    "\n",
    "national_table = pd.crosstab(index=national[0], columns=\"count\")\n",
    "minnesota_table = pd.crosstab(index=minnesota[0], columns=\"count\")\n",
    "\n",
    "print( \"National\")\n",
    "print(national_table)\n",
    "print(\" \")\n",
    "print( \"Minnesota\")\n",
    "print(minnesota_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J41ClZoABWfG"
   },
   "source": [
    "Chi-squared tests are based on the so-called chi-squared statistic. You calculate the chi-squared statistic with the following formula:\n",
    "\n",
    ">$sum((observed−expected)^2/expected)$\n",
    "\n",
    "\n",
    "In the formula, observed is the actual observed count for each category and expected is the expected count based on the distribution of the population for the corresponding category. \n",
    "\n",
    "Let's calculate the chi-squared statistic for our data to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8TGcyKNBWfG"
   },
   "outputs": [],
   "source": [
    "observed = minnesota_table\n",
    "\n",
    "# Calculate population ratios\n",
    "\n",
    "\n",
    "# Calculate expected counts\n",
    "\n",
    "\n",
    "# Calculate the chi-square statistics as per the formula above and print it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nfRq0J3BWfI"
   },
   "source": [
    "**Note:** The chi-squared test assumes none of the expected counts are less than 5.\n",
    "\n",
    "Similar to the t-test where we compared the t-test statistic to a critical value based on the t-distribution to determine whether the result is significant, in the chi-square test we compare the chi-square test statistic to a critical value based on the chi-square distribution. \n",
    "\n",
    "The scipy library shorthand for the chi-square distribution is chi2. \n",
    "\n",
    "Let's use this knowledge to find the critical value for 95% confidence level and check the p-value of our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKe59gBlBWfJ"
   },
   "outputs": [],
   "source": [
    "crit = stats.chi2.ppf(q = 0.95, # Find the critical value for 95% confidence*\n",
    "                      df = 4)   # Df = number of variable categories - 1\n",
    "\n",
    "print(\"Critical value\")\n",
    "print(crit)\n",
    "\n",
    "p_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # Find the p-value\n",
    "                             df=4)\n",
    "print(\"P value\")\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1Y3GtdFBWfL"
   },
   "source": [
    "**Note:** we are only interested in the right tail of the chi-square distribution. Read more on this [here](https://en.wikipedia.org/wiki/Chi-squared_distribution).\n",
    "\n",
    "Since our chi-squared statistic exceeds the critical value, we'd reject the null hypothesis that the two distributions are the same.\n",
    "\n",
    "You can carry out a chi-squared goodness-of-fit test automatically using the scipy function scipy.stats.chisquare():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d61AphnHBWfL"
   },
   "outputs": [],
   "source": [
    "stats.chisquare(f_obs= observed,   # Array of observed counts\n",
    "                f_exp= expected)   # Array of expected counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHugqtqwBWfN"
   },
   "source": [
    "The test results agree with the values we calculated above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "daiX6zNgBWfO"
   },
   "source": [
    "### Chi-Squared Test of Independence\n",
    "***\n",
    "Independence is a key concept in probability that describes a situation where knowing the value of one variable tells you nothing about the value of another. \n",
    "\n",
    "For instance, the month you were born probably doesn't tell you anything which web browser you use, so we'd expect birth month and browser preference to be independent. \n",
    "\n",
    "On the other hand, your month of birth might be related to whether you excelled at sports in school, so month of birth and sports performance might not be independent.\n",
    "\n",
    "The chi-squared test of independence tests whether two categorical variables are independent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwQLvwVvBWfO"
   },
   "source": [
    "## John's final experiments\n",
    "***\n",
    "John wants to test if knowing `LandContour` which is the overall flatness of the property tells him anything about the price\n",
    "\n",
    " -  He has divided the `SalePrice` in three buckets - High, medium, low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOEpFOy8BWfO"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as sp\n",
    "def compute_freq_chi2(x,y):\n",
    "    \"\"\"This function will compute frequency table of x an y\n",
    "    Pandas Series, and use the table to feed for the contigency table\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    x,y : Pandas Series, must be same shape for frequency table\n",
    "    \n",
    "    Return:\n",
    "    -------\n",
    "    None. But prints out frequency table, chi2 test statistic, and \n",
    "    p-value\n",
    "    \"\"\"\n",
    "    freqtab = pd.crosstab(x,y)\n",
    "    print(\"Frequency table\")\n",
    "    print(\"============================\")\n",
    "    print(freqtab)\n",
    "    print(\"============================\")\n",
    "    chi2,pval,dof,expected = sp.chi2_contingency(freqtab)\n",
    "    print(\"ChiSquare test statistic: \",chi2)\n",
    "    print(\"p-value: \",pval)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIQfAUwcBWfR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price = pd.qcut(data['SalePrice'], 3, labels = ['High', 'Medium', 'Low'])\n",
    "compute_freq_chi2(data.LandContour, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5HkJchZBWfS"
   },
   "source": [
    "* The low p-value tells us that the two variables aren't independent and knowing the `LandContour` of a house does tells us something about its `SalePrice`.\n",
    "* The frequency distribution reflects this.\n",
    "* Houses that are Near Flat/Level(Lvl) have an equal distribution of `SalePrice`.\n",
    "* On the other hand houses that are at a Hillside i.e., Significant slope from side to side (HLS) have almost thrice as much houses with low price than high prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG-rLTuH-EpR"
   },
   "source": [
    "## In Class Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('weather_2012.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1\n",
    "\n",
    "- Consider a sample of `Visibility (km)` of 2000 observations randomly and print it's mean and then check it with the entire population mean of `Visibility (km)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2\n",
    "Find the 95% confidence interval for the Temperature column in the weather dataset for a sample size of 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3\n",
    "\n",
    "- For the Weather dataset, let the null and alternate hypothesis be as follows:\n",
    "    Null Hypothesis: The mean of Temperature when it is snow ('Snow Showers') is the same as the population mean.\n",
    "    Alternate Hypothesis: The mean of Temperature when it is snow ('Snow Showers') is different than the population \n",
    "mean.\n",
    "\n",
    "Test the above hypothesis at 90% significance level and print True if we do not reject the null hypothesis. Calculate two-sided test for the null hypothesis using [stats.ttest_1samp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code starts here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "learner_inferential_stats_in_class_slides.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
